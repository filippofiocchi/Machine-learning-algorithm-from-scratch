{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.4 64-bit ('Tensorflow-ler0PpP0')",
   "display_name": "Python 3.6.4 64-bit ('Tensorflow-ler0PpP0')",
   "metadata": {
    "interpreter": {
     "hash": "07df06258ffa39de7418b7b98983ac1411cc626ca44a2859d6ba720f8843c017"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import Counter\n"
   ]
  },
  {
   "source": [
    "# Cross entropy loss :\n",
    "$$L = \\sum_{c} p_c * log_2(p_c)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Cross_entropy methos, globally, we could also use the Gini, instead of cross entropy as loss function\n",
    "def entropy_method(y):\n",
    "    class_count = np.bincount(y) #calcluate the number of occurency of all class label\n",
    "    proportions = class_count/len(y)\n",
    "    return - np.sum([p*np.log2(p) for p in proportions if p > 0])\n"
   ]
  },
  {
   "source": [
    "# Node class\n",
    "Now define a class that will store the information for our node, if we are in the middle , so the node is not a leaf, we want to store the best feature to split and the treshold to use.\n",
    "We want to store alse the left and right child node, becase we need the to calculate the loss of the split, if we are at a leaf node we store the most common class labels that will be the value of that leaf.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "  #we just store the infromation      \n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value \n",
    "#now we define a function that tell us if our node is a leaf node \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "#if the node is a leaf, it will return a value so self.value will not be None, so this function will return True, meaning that the node is a leaf\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Decision Tree classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree: \n",
    "    #in practice now we do a greedy search over oll the features but we can also loop only in a subset of feature in random way --> random forest\n",
    "    def __init__(self, min_sample_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_sample_split = min_sample_split #if there are too few element in a node stop split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None #the starting node\n",
    "\n",
    "# Now we define all the helper funtion that our fit and predict methonds need\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y) # similar to the np.bincount\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "      #very most common tuple in the list, and we want the first elment of that tuple, in the touple there is the value store and also the number of occurencies, and we only want the value\n",
    "        return most_common\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split the tree and save the childres\n",
    "    def _split(self, X_column, split_threh):\n",
    "        left_idx = np.argwhere(X_column <= split_threh).flatten()\n",
    "        right_idx = np.argwhere(X_column > split_threh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# infromation gain to use the search for the best split\n",
    "    def _information_gain(self, y, X_column, split_threh):\n",
    "        #parent entropy\n",
    "        parent_en = entropy_method(y)\n",
    "        #generate split\n",
    "        left_idx, right_idx = self._split(X_column, split_threh)\n",
    "        \n",
    "        if len(left_idx)==0 or len(right_idx)==0 : \n",
    "            # we don't gain information so we return 0\n",
    "            return 0\n",
    "        \n",
    "        #weighted average of the cross entropy of the childs\n",
    "        n = len(y)\n",
    "        n_l = len(left_idx)\n",
    "        n_r = len(right_idx)\n",
    "        e_l,e_r = entropy_method(y[left_idx]), entropy_method(y[right_idx])\n",
    "        child_en = (n_l/n)*e_l + (n_r/n)*e_r\n",
    "\n",
    "        #return information gain\n",
    "        info_gain = parent_en - child_en\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the function for our greedy search\n",
    "    def _best_criteria(self, X, y, feat_idxs): \n",
    "        #go aver all the feaures and all the features' values and calculate the information gain\n",
    "        best_gain = -1\n",
    "        split_idx, split_threh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:,feat_idx] #look only at the column of the current feature\n",
    "            thresholds = np.unique(X_column) #go over all possible threshold but don't check the same value twice\n",
    "            for threshold in thresholds :\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "                #we want to maximize the gain\n",
    "                if gain > best_gain: \n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threh = threshold\n",
    "        #return the best split index(the feature to split) and\n",
    "        # the best split threshold(who to split the feature cloumn) in order\n",
    "        #to obain the greater gain in information\n",
    "        return split_idx, split_threh  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #our main helper funtion\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "    # depth is 0 in the beginning but we need to keep track of that\n",
    "\n",
    "        n_sample, n_feature = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "    # stopping criteria \n",
    "        #if one of the followinf is true we return the leaf node with the value of the most common label\n",
    "        if (depth >= self.max_depth or n_labels == 1  or n_sample < self.min_sample_split): \n",
    "                                      \n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        \n",
    "     #if the node did't meet the stopping criterias:\n",
    "        feat_idxs = np.random.choice(n_feature, self.n_feats, replace=False)\n",
    "        #(the range in wich made the choice, the length of the vector of choice=number of choice to make, we don't want to have the same index multiple time )\n",
    "\n",
    "        #greedy search--> split to obtain the best gain in infromation\n",
    "        best_feat, best_thresh = self._best_criteria(X,y,feat_idxs)\n",
    "        left_idx, right_idx = self._split(X[:, best_feat], best_thresh) \n",
    "        \n",
    "        #split inside the best column by recursively recoll grow_tree inside the cloumn split(the new node)\n",
    "        left = self._grow_tree(X[left_idx,:], y[left_idx], depth+1)\n",
    "        right = self._grow_tree(X[right_idx,:], y[right_idx], depth+1)\n",
    "        return Node(best_feat, best_thresh, left, right) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper funtion for the predict method\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "\n",
    "\n",
    "# Define fit and predict methods\n",
    "    def fit(self,X,y):\n",
    "        #Now if we don't specify how many feture we want, we use all the featueres in X \n",
    "        # that correspond to the second dimension X[1] bc X = n x d where d are the number of all the features\n",
    "        #is we specify the n_feats we take the minimum btw n_feats and the number of all the features in X\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        #grow tree\n",
    "        self.root = self._grow_tree(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        #the root is where we start\n",
    "        return np.array([self._traverse_tree(x,self.root) for x in X] )\n",
    "       \n"
   ]
  },
  {
   "source": [
    "# Test our Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "\n",
    "print (\"Accuracy:\", acc)"
   ]
  }
 ]
}